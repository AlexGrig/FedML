{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f21aaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexgrig/miniconda/envs/fedml/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from pathlib import Path \n",
    "import shutil\n",
    "\n",
    "import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ee72f8-5510-4e1f-9175-ed93083563a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1914752\n",
      "drwxrwxr-x@  3 alexgrig  staff         96 Nov 28  2018 \u001b[34m.\u001b[m\u001b[m/\n",
      "drwxr-xr-x  10 alexgrig  staff        320 Oct 12 12:21 \u001b[34m..\u001b[m\u001b[m/\n",
      "-rw-r--r--@  1 alexgrig  staff  971880155 Nov 28  2018 all_data_0_niid_0_keep_10_train_9.json\n"
     ]
    }
   ],
   "source": [
    "%ls -al /Users/alexgrig/data/fedml/mnist/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b9deff-020b-4bd4-88ac-3369f893b4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExisting Functions:\\n\\nlist_datasets\\nload_dataset_builder # Explore dataset\\nload_dataset # Actually load it\\nget_dataset_split_names # Explore dataset. Get split names\\nget_dataset_config_names # Explore dataset. Get config (subdatasets) names\\n\\nconcatenate_datasets\\nremove_columns\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Existing Functions:\n",
    "\n",
    "list_datasets\n",
    "load_dataset_builder # Explore dataset\n",
    "load_dataset # Actually load it\n",
    "get_dataset_split_names # Explore dataset. Get split names\n",
    "get_dataset_config_names # Explore dataset. Get config (subdatasets) names\n",
    "\n",
    "concatenate_datasets\n",
    "remove_columns\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1025d844-b17d-4a5e-afac-1a3c82db405e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fashion_mnist',\n",
       " 'mnist',\n",
       " 'Fraser/mnist-text-default',\n",
       " 'Fraser/mnist-text-no-spaces',\n",
       " 'Fraser/mnist-text-small',\n",
       " 'cgarciae/point-cloud-mnist',\n",
       " 'lewtun/mnist-preds',\n",
       " 'msollami-sf/processed_mnist',\n",
       " 'mweiss/fashion_mnist_corrupted',\n",
       " 'flexthink/audiomnist',\n",
       " 'autoevaluate/mnist-sample',\n",
       " 'chrisjay/mnist-adversarial-dataset',\n",
       " 'arize-ai/fashion_mnist_label_drift',\n",
       " 'chrisjay/test-mnist',\n",
       " 'arize-ai/fashion_mnist_quality_drift',\n",
       " 'chrisjay/test-mnist-2',\n",
       " 'chrisjay/test-mnist-3',\n",
       " 'chrisjay/test-mnist-4',\n",
       " 'chrisjay/test-mnist-5',\n",
       " 'Depshad/ood_mnist_c']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dd for dd in datasets.list_datasets() if 'mnist' in dd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e43f217-3a56-4fa2-a01a-3d995ccba5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset mnist (/Users/alexgrig/.cache/huggingface/datasets/mnist/mnist/1.0.0/fda16c03c4ecfb13f165ba7e29cf38129ce035011519968cdaf74894ce91c9d4)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 451.78it/s]\n",
      "Found cached dataset mnist (/Users/alexgrig/.cache/huggingface/datasets/mnist/mnist/1.0.0/fda16c03c4ecfb13f165ba7e29cf38129ce035011519968cdaf74894ce91c9d4)\n"
     ]
    }
   ],
   "source": [
    "mnist = datasets.load_dataset('mnist')\n",
    "ds_test = datasets.load_dataset('mnist', split='test[0:5]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d37028-da09-4380-b803-a35e6a83d2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['image', 'label'],\n",
       "         num_rows: 60000\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['image', 'label'],\n",
       "         num_rows: 10000\n",
       "     })\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['image', 'label'],\n",
       "     num_rows: 5\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be816573-1b66-4e10-9f63-63c4425bb19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Image(decode=True, id=None),\n",
       " 'label': ClassLabel(names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7ea66-41ea-4492-9c01-69e986bd7089",
   "metadata": {},
   "source": [
    "## Filter on labels and substitute labels (make the problem binary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92b0ee45-278b-45d6-af29-ccecf29e2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_labels = [3,8]\n",
    "new_labels = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42fdd73b-4fdc-4518-8de0-10fb4608f289",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 11/12 [00:00<00:00, 11.62ba/s]\n",
      " 50%|███████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                               | 1/2 [00:00<00:00,  4.52ba/s]\n"
     ]
    }
   ],
   "source": [
    "mnist = mnist.filter(lambda sample: sample['label'] in use_labels, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eac229b-55cf-4eca-9696-1eab80187a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 11982\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1984\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24ac0793-a6a6-4bd5-b14b-a2a97362d5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 8]), array([3, 8]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check filtering\n",
    "np.unique( mnist['test']['label'] ), np.unique( mnist['train']['label'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ea2f6-23f6-408b-a012-d3b03eb724d8",
   "metadata": {},
   "source": [
    "## Map lables to new labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d001287-87c1-4929-835a-7c034cad6174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function substitute at 0x7f8ab4380a70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11982/11982 [00:00<00:00, 12803.40ex/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1984/1984 [00:00<00:00, 12515.79ex/s]\n"
     ]
    }
   ],
   "source": [
    "def substitute(sample):\n",
    "    \n",
    "    sample['label'] = (new_labels[0] if (sample['label']==use_labels[0]) else new_labels[1])\n",
    "    return sample\n",
    "\n",
    "mnist = mnist.map(substitute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6d7bd83-4fbe-41c0-be29-7e4a93404a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([0, 1]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check substitution\n",
    "np.unique( mnist['test']['label'] ), np.unique( mnist['train']['label'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b94dc-ed8d-461a-b4eb-26108e5b6709",
   "metadata": {},
   "source": [
    "## Split train part into val and train parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a97fc0c-6595-4f85-98c8-c7c7b03c1676",
   "metadata": {},
   "source": [
    "### Make validation part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8739b5f6-d08b-4e05-ac56-177fb4da9c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_validation = False\n",
    "\n",
    "test_size = 0.15\n",
    "stratify_by_column='label'\n",
    "shuffle=True\n",
    "seed=47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96295f2e-1579-4868-a98b-349f5bb48e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 11982\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1984\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_train_val_split(ds, test_size=0.15, stratify_by_column='label', shuffle=True, seed=47):\n",
    "    d_dict = ds.train_test_split(test_size=test_size, stratify_by_column=stratify_by_column, shuffle=shuffle, seed=seed)\n",
    "    \n",
    "    train_train = d_dict['train']\n",
    "    train_train.split._name = 'train_train'\n",
    "    \n",
    "    train_val = d_dict['test']\n",
    "    train_val.split._name = 'train_val'\n",
    "    \n",
    "    return train_train, train_val\n",
    "\n",
    "# divide onto train and val\n",
    "if make_validation:\n",
    "    train_train, train_val = make_train_val_split(mnist['train'], test_size=test_size, stratify_by_column=stratify_by_column, shuffle=shuffle, seed=seed)\n",
    "    mnist = datasets.DatasetDict({'train_train': train_train, 'train_val': train_val, 'test': mnist['test']})\n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25afd717-b75d-4349-a05a-c7fea198617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "to_image = transforms.ToPILImage()\n",
    "\n",
    "def split_image(image, parts=2):\n",
    "    \"\"\"\n",
    "    Splits the image onto several parts. Has not been tested when the number of parts larger than 5.\n",
    "    So, there may be border effects.\n",
    "    \"\"\"\n",
    "    \n",
    "    split_dim = 1\n",
    "    split_dim_size = image.shape[split_dim] \n",
    "    \n",
    "    part_size = int( np.ceil(split_dim_size / parts) )\n",
    "    split_points = np.arange(0, split_dim_size, part_size)\n",
    "    #print(split_points)\n",
    "                             \n",
    "    image_parts = []\n",
    "    start_points = list(split_points[0:])\n",
    "    end_points = list(split_points[1:]) + [split_dim_size+1,]                          \n",
    "    for st,en in zip(start_points, end_points):\n",
    "        part = image[st:en,:] if (split_dim == 0) else image[:,st:en]\n",
    "        image_parts.append(part)\n",
    "        \n",
    "    return image_parts\n",
    "\n",
    "\n",
    "def split_vertically(sample, parts=3, split_feature='image', part_prefix='image_part'):\n",
    "    \n",
    "    image = sample[split_feature] # make sure it is decoded from the storing format\n",
    "    #import pdb; pdb.set_trace()\n",
    "    timage = to_tensor(image)[0,:,:]\n",
    "    \n",
    "    image_parts = { f'{part_prefix}_{ii}':to_image(img) for ii,img in enumerate(split_image(timage, parts=parts)) }\n",
    "    \n",
    "    return image_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae74ef-0563-4117-83d9-198cdc8fd2b7",
   "metadata": {},
   "source": [
    "## Test the splitting over a features into parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd451662-e9d8-465a-85fc-24f3fc348e7a",
   "metadata": {},
   "source": [
    "### Set splitting parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b9cbe20-ce28-46f6-9000-6e4e4c071094",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_num = 3\n",
    "part_prefix = 'image_part'\n",
    "split_feature='image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3904ba27-1550-4fd8-81e9-ebddf87d505f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1984/1984 [00:01<00:00, 1851.82ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'image_part_0', 'image_part_1', 'image_part_2'],\n",
       "    num_rows: 1984\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test_splitted = mnist['test'].map(partial(split_vertically, parts=parts_num, part_prefix=part_prefix,\n",
    "                                               split_feature=split_feature), remove_columns='image') #, features='image')\n",
    "mnist_test_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2d91ba3-c4da-4849-ac5e-9920b615e7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAoAAAAcCAAAAAB91nHnAAAAN0lEQVR4nGNgoBQwQmnRmACY0Pa/f5igTBGmtzDRwOlGFFtGTcC9Fs5sgbuXQZSRBcY8rI5dIwA+9gjQrhUhgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=10x28>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_test_splitted[1]['image_part_0'] #, mnist_test_splitted[1]['image_part_1'], mnist_test_splitted[1]['image_part_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f31e641-05b5-48d8-ac8d-1524c455d571",
   "metadata": {},
   "source": [
    "## Split the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d39226a7-34f7-4b92-bd6f-a9f8a97a0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_dict(ds_dict, parts=3, split_feature='image', part_prefix='image_part'):\n",
    "    \n",
    "    # we assume that datasets in the dict have the same features\n",
    "    common_features = [ff for ff in list(ds_dict.values())[0].features.keys() if (ff != split_feature)]\n",
    "    \n",
    "    splited_dss = {}\n",
    "    for val, ds in ds_dict.items():\n",
    "        ds_splited = ds.map(partial(split_vertically, parts=parts_num, part_prefix=part_prefix,\n",
    "                                               split_feature=split_feature), remove_columns=split_feature)\n",
    "        splited_dss[val] = ds_splited\n",
    "        \n",
    "    splitted_features = [ff for ff in splited_dss[list(splited_dss.keys())[0]].features.keys() if ff not in common_features]\n",
    "    \n",
    "    # seems that only the remove feature we can use here\n",
    "    parts = []\n",
    "    for ff in splitted_features:\n",
    "        features_to_remove = [ee for ee in splitted_features if ee!=ff]\n",
    "        new_ds_dict = datasets.DatasetDict()\n",
    "        \n",
    "        for val, ds in splited_dss.items():\n",
    "            new_ds_dict[val] = ds.remove_columns(features_to_remove)\n",
    "            \n",
    "        parts.append(new_ds_dict)\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2d5d0dc-716a-4f9c-a415-a8a884947467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11982/11982 [00:09<00:00, 1248.88ex/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1984/1984 [00:01<00:00, 1939.64ex/s]\n"
     ]
    }
   ],
   "source": [
    "rr = split_dataset_dict(mnist, parts=3, split_feature='image', part_prefix='image_part')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c01addd-374c-42a1-b350-e62e0ddf6fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(1, len(rr)):\n",
    "    if 'label' in rr[ii]['test'].column_names:\n",
    "        rr[ii] = rr[ii].remove_columns('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a49ce0a-de90-4aa8-81cc-43ce5aed6909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['label', 'image_part_0'],\n",
       "         num_rows: 11982\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['label', 'image_part_0'],\n",
       "         num_rows: 1984\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['image_part_1'],\n",
       "         num_rows: 11982\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['image_part_1'],\n",
       "         num_rows: 1984\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['image_part_2'],\n",
       "         num_rows: 11982\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['image_part_2'],\n",
       "         num_rows: 1984\n",
       "     })\n",
       " })]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40affbe8-4443-4b30-b787-b0516b190c30",
   "metadata": {},
   "source": [
    "## Save the whole dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab59765-515d-42fa-8c1e-e997704e14f7",
   "metadata": {},
   "source": [
    "### Saving parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe3596ff-2231-4d3f-8cb2-2766ab64f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/alexgrig/data/mnist_binary38_parts3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "611bcff0-9540-485c-a835-5c9ff9dd628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_splitted_dataset(ds_list, path, part_dir_name='part_', clean_dir=False):\n",
    "    path = Path(path)\n",
    "    \n",
    "    if not path.exists():\n",
    "        path.mkdir()\n",
    "    \n",
    "    #clean the directory:\n",
    "    if clean_dir:\n",
    "        for pt in path.glob('*'):\n",
    "            if pt.is_file():\n",
    "                pt.unlink()\n",
    "            elif pt.is_dir():\n",
    "                shutil.rmtree(pt)\n",
    "\n",
    "    for ii,ds in enumerate(ds_list):\n",
    "        part_dir =  f'{part_dir_name}{ii}'\n",
    "        \n",
    "        part_path = path / part_dir\n",
    "        if part_path.exists():\n",
    "            raise IOError('Directory already exists')\n",
    "            \n",
    "        part_path.mkdir()\n",
    "        \n",
    "        ds.save_to_disk(part_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83ab12f0-9242-413a-b23e-fe6c9abaa598",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_splitted_dataset(rr, path=save_dir, clean_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "205db5d8-45b5-46c7-8a47-5399fb5a1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_splitted_part(path, part_no, part_prefix = 'part_', split_feature_prefix='image', new_name_split_feature=None):\n",
    "    path = Path(path)\n",
    "    \n",
    "    part_path = path / f'{part_prefix}{part_no}'\n",
    "    \n",
    "    part_ds = datasets.load_from_disk(part_path)\n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    \n",
    "    if new_name_split_feature is not None:\n",
    "        for kk,ds in part_ds.items():\n",
    "            rename_feature = [str(ff) for ff in list(ds.features) if split_feature_prefix in str(ff)][0]\n",
    "            \n",
    "            part_ds[kk] = ds.rename_column(rename_feature, new_name_split_feature)\n",
    "            \n",
    "    return part_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78d4836e-9609-4adb-af08-4da776b5aa43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image_part_2'],\n",
       "        num_rows: 11982\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image_part_2'],\n",
       "        num_rows: 1984\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_splitted_part(save_dir, 2, part_prefix = 'part_', split_feature_prefix='image', new_name_split_feature=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "30c9dd84-2733-4e77-8701-e2f50af68b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 11982, 'test': 1984}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr[0].num_rows #['train']['image_part_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8cde430a-23cc-4835-8897-361dab6bc4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAcCAAAAAB5I6HaAAAANUlEQVR4nGNgIAWsY4My/szjgzL+WDMwMDAwMMHV/FnDDhV58hMqAlXLdOE3VI0MSbbSCgAAaioLE2edyBcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=8x28>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr[2]['train']['image_part_2'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5eb6a88e-d8f6-4884-8099-f10fa4215296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rr[0]['train']['label'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "68f9aa2f-4114-46ef-805f-afe802387967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(rr[0]['train']['label'][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "06613d50-b2a4-4c12-b4ba-899f165ef43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rr[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7bfcc-7f62-491d-93c8-4353a9bfb5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedml",
   "language": "python",
   "name": "fedml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
